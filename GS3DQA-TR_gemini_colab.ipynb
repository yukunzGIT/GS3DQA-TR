{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2024 Google LLC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeadDkMiISin"
   },
   "source": [
    "# Get started with the Gemini API: Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lEXQ3OwKIa-O"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://ai.google.dev/gemini-api/docs/get-started/python\"><img src=\"https://ai.google.dev/static/site-assets/images/docs/notebook-site-button.png\" height=\"32\" width=\"32\" />View on Google AI</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/get-started/python.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/get-started/python.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOxMUKTxR-_j"
   },
   "source": [
    "This quickstart demonstrates how to use the Python SDK for the Gemini API, which gives you access to Google's Gemini large language models. In this quickstart, you will learn how to:\n",
    "\n",
    "1. Set up your development environment and API access to use Gemini.\n",
    "2. Generate text responses from text inputs.\n",
    "3. Generate text responses from multimodal inputs (text and images).\n",
    "4. Use Gemini for multi-turn conversations (chat).\n",
    "5. Use embeddings for large language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9__zr1nSBpE"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "You can run this quickstart in [Google Colab](https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemini-api/docs/get-started/python.ipynb), which runs this notebook directly in the browser and does not require additional environment configuration.\n",
    "\n",
    "Alternatively, to complete this quickstart locally, ensure that your development environment meets the following requirements:\n",
    "\n",
    "-  Python 3.9+\n",
    "-  An installation of `jupyter` to run the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFPBKLapSCkM"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFNV1e3ASJha"
   },
   "source": [
    "### Install the Python SDK\n",
    "\n",
    "The Python SDK for the Gemini API, is contained in the [`google-generativeai`](https://pypi.org/project/google-generativeai/) package. Install the dependency using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8KLBO0XwfQam",
    "outputId": "927fc588-2890-48bb-cc40-ab414ba08cce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6j1C950fcpg",
    "outputId": "57a3987f-eca0-40e6-b192-f401fb59ed9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive\n",
      "MyDrive\n"
     ]
    }
   ],
   "source": [
    " %cd drive\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m6Bzji9UgFEJ",
    "outputId": "3d0d5f20-7310-4502-fdf9-dc69576f3291"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/FinRAG\n",
      " cache\t\t\t\t  FinRAD_preprocess_gemini_colab.ipynb\t output\n",
      "'Copy of FinRAG Poster.gslides'   generator\t\t\t\t retriever_external\n",
      " dataset\t\t\t  generator_train_test.ipynb\t\t retriever_internal\n"
     ]
    }
   ],
   "source": [
    " %cd MyDrive/FinRAG\n",
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9OEoeosRTv-5",
    "outputId": "8f0522f2-e02a-4159-c847-3dd0b5e99e36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.7/150.7 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.1/679.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "#!pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCFF5VSTbcAR"
   },
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vRC2HngneEeQ"
   },
   "source": [
    "Import the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TS9l5igubpHO"
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import textwrap\n",
    "\n",
    "import google.generativeai as genai\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d10c38a5c91f"
   },
   "outputs": [],
   "source": [
    "# Used to securely store your API key\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_daGs-EzmdhP"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import json\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHYFrFPjSGNq"
   },
   "source": [
    "### Setup your API key\n",
    "\n",
    "Before you can use the Gemini API, you must first obtain an API key. If you don't already have one, create a key with one click in Google AI Studio.\n",
    "\n",
    "<a class=\"button button-primary\" href=\"https://makersuite.google.com/app/apikey\" target=\"_blank\" rel=\"noopener noreferrer\">Get an API key</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHhsUxDTdw0W"
   },
   "source": [
    "In Colab, add the key to the secrets manager under the \"🔑\" in the left panel. Give it the name `GOOGLE_API_KEY`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmSlTHXxb5pV"
   },
   "source": [
    "Once you have the API key, pass it to the SDK. You can do this in two ways:\n",
    "\n",
    "* Put the key in the `GOOGLE_API_KEY` environment variable (the SDK will automatically pick it up from there).\n",
    "* Pass the key to `genai.configure(api_key=...)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab9ASynfcIZn"
   },
   "outputs": [],
   "source": [
    "# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.\n",
    "GOOGLE_API_KEY=userdata.get('gemini-paid')\n",
    "\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ssbTMNVSMd-"
   },
   "source": [
    "## List models\n",
    "\n",
    "Now you're ready to call the Gemini API. Use `list_models` to see the available Gemini models:\n",
    "\n",
    "* `gemini-pro`: optimized for text-only prompts.\n",
    "* `gemini-pro-vision`: optimized for text-and-images prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vr8eCStMgyKD"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "#userdata.get('gemini-paid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 446
    },
    "id": "QvvWFy08e5c5",
    "outputId": "f6a004f4-b893-44a9-998a-29a1754e7f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-exp-1114\n"
     ]
    }
   ],
   "source": [
    "for m in genai.list_models():\n",
    "  if 'generateContent' in m.supported_generation_methods:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTl5NjtrhA0J"
   },
   "source": [
    "Note: For detailed information about the available models, including their capabilities and rate limits, see [Gemini models](https://ai.google.dev/models/gemini). There are options for requesting [rate limit increases](https://ai.google.dev/docs/increase_quota). The rate limit for Gemini-Pro models is 60 requests per minute (RPM).\n",
    "\n",
    "The `genai` package also supports the PaLM  family of models, but only the Gemini models support the generic, multimodal capabilities of the `generateContent` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LZfoK3I3hu6V"
   },
   "source": [
    "## Generate text from text inputs\n",
    "\n",
    "For text-only prompts, use the `gemini-pro` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bcfnGEviwTI"
   },
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "#model = genai.GenerativeModel('gemini-1.0-pro')\n",
    "\n",
    "safety_settings = [\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "        \"threshold\": \"BLOCK_NONE\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WR_2A_sxk8sK"
   },
   "source": [
    "The `generate_content` method can handle a wide variety of use cases, including multi-turn chat and multimodal input, depending on what the underlying model supports. The available models only support text and images as input, and text as output.\n",
    "\n",
    "In the simplest case, you can pass a prompt string to the <a href=\"https://ai.google.dev/api/python/google/generativeai/GenerativeModel#generate_content\"><code>GenerativeModel.generate_content</code></a> method:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbrR-n_qlpFd"
   },
   "source": [
    "In simple cases, the `response.text` accessor is all you need. To display formatted Markdown text, use the `to_markdown` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "id": "G-zBkueElVEO",
    "outputId": "08b121ef-a895-465c-f921-4034b00708cf"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> As a large language model, I can't tell you the meaning of life. That's a question philosophers and theologians have grappled with for centuries! \n",
       "> \n",
       "> The meaning of life is a personal and individual question. There's no one right answer.  It's up to each person to decide what gives their life meaning. \n",
       "> \n",
       "> Here are some things to consider:\n",
       "> \n",
       "> * **Your values:** What is important to you? What do you believe in?\n",
       "> * **Your purpose:** What do you want to accomplish in life? What impact do you want to make?\n",
       "> * **Your experiences:** What brings you joy? What challenges you? What makes you feel fulfilled?\n",
       "> \n",
       "> Ultimately, the meaning of life is what you make it. It's about finding what gives your life purpose and makes you feel fulfilled. \n",
       "> \n",
       "> If you're struggling with this question, it might be helpful to talk to a trusted friend, family member, or therapist. They can offer support and guidance as you explore what gives your life meaning. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to_markdown(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hot92CR-lyuZ"
   },
   "source": [
    "# Now, use gemini 1.5 pro lastest for zero-shot!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "72ytxAHU_Mq_"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install dependencies\n",
    "!pip install -q evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8voWSn-M_sRz"
   },
   "outputs": [],
   "source": [
    "# Cell 2: Imports & Authentication\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "import PIL.Image\n",
    "from evaluate import load as load_metric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4agHO0h_lVL"
   },
   "outputs": [],
   "source": [
    "# Cell 3: Image‐sampling utilities\n",
    "\n",
    "def sample_method1(color_dir):\n",
    "    \"\"\"\n",
    "    Evenly select 10 color frames from color_dir (e.g. 0.jpeg…N.jpeg).\n",
    "    \"\"\"\n",
    "    frames = sorted(\n",
    "        [f for f in os.listdir(color_dir)\n",
    "         if f.endswith('.jpeg') or f.endswith('.jpg')],\n",
    "        key=lambda x: int(os.path.splitext(x)[0])\n",
    "    )\n",
    "    N = len(frames)\n",
    "    # 10 indices spaced across [0, N-1]\n",
    "    idxs = [round(i * (N - 1) / 9) for i in range(10)]\n",
    "    return [os.path.join(color_dir, frames[i]) for i in idxs]\n",
    "\n",
    "\n",
    "def sample_method2(color_dir, depth_dir):\n",
    "    \"\"\"\n",
    "    Use the images from the 10 color frames (method1),\n",
    "    then include their corresponding depth (.png) files.\n",
    "    \"\"\"\n",
    "    chosen = sample_method1(color_dir)\n",
    "    #chosen = random.sample(ten, 5)\n",
    "    depths = [\n",
    "        os.path.join(depth_dir, os.path.splitext(os.path.basename(p))[0] + '.png')\n",
    "        for p in chosen\n",
    "    ]\n",
    "    return chosen + depths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "exOUhH58_UY0"
   },
   "outputs": [],
   "source": [
    "# Cell 4: Load test questions & answers\n",
    "with open('data/splits/test_qa.json', 'r') as f:\n",
    "    test_entries = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzZMl8DC_5Iv"
   },
   "outputs": [],
   "source": [
    "# Cell 5: Zero‐shot query helper\n",
    "\n",
    "def ask_gemini(question: str, image_paths: list[str]) -> str:\n",
    "    \"\"\"\n",
    "    Sends images + question to Gemini and returns the answer text.\n",
    "    \"\"\"\n",
    "    question = \"Given the attached images, \" + question\n",
    "    parts = []\n",
    "    for path in image_paths:\n",
    "        img = PIL.Image.open(path)\n",
    "        parts.append(img)\n",
    "\n",
    "    parts.append(question)\n",
    "    response = model.generate_content(parts)\n",
    "    return response.text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zIJaCWTeESlo"
   },
   "outputs": [],
   "source": [
    "# Cell 6: Evaluation loop & metrics\n",
    "\n",
    "meteor = load_metric('meteor')\n",
    "\n",
    "def evaluate_method(method: int):\n",
    "    preds, refs = [], []\n",
    "    ans_types, spat_types = [], []\n",
    "\n",
    "    for entry in test_entries:\n",
    "        sid      = entry['scene_id']   # e.g. \"scene0581_00\"\n",
    "        color_dir= f\"rgbd-{sid}/color\"\n",
    "        depth_dir= f\"rgbd-{sid}/depth\"\n",
    "\n",
    "        # 1 or 2 → choose sampling strategy\n",
    "        if method == 1:\n",
    "            imgs = sample_method1(color_dir)\n",
    "        else:\n",
    "            imgs = sample_method2(color_dir, depth_dir)\n",
    "\n",
    "        # zero‐shot answer\n",
    "        ans = ask_gemini(entry['question'], imgs)\n",
    "        preds.append(ans)\n",
    "        refs.append(entry['answer'])\n",
    "\n",
    "        # record categories\n",
    "        atype = entry.get('question_type', entry.get('answer_type','Other'))\n",
    "        ans_types.append(atype)\n",
    "        spat = entry.get('spatial_subtask',\n",
    "                         atype if atype in\n",
    "                           ['aggregation','placement','spatial','viewpoint']\n",
    "                         else 'none')\n",
    "        spat_types.append(spat)\n",
    "\n",
    "    # overall metrics\n",
    "    em_overall   = sum(p==r for p,r in zip(preds,refs)) / len(refs) * 100\n",
    "    met_overall  = meteor.compute(\n",
    "        predictions=preds,\n",
    "        references=[[r] for r in refs]\n",
    "    )['meteor'] * 100\n",
    "\n",
    "    print(f\"\\n=== Method {method} Results ===\")\n",
    "    print(f\"Overall EM:     {em_overall:.2f}%\")\n",
    "    print(f\"Overall METEOR: {met_overall:.2f}%\")\n",
    "\n",
    "    # per-answer‐type\n",
    "    print(\"\\n-- Answer‐Type Breakdown --\")\n",
    "    for cat in ['Y/N','Color','Number','Other']:\n",
    "        idxs = [i for i,t in enumerate(ans_types) if t==cat]\n",
    "        if not idxs: continue\n",
    "        em_c = sum(preds[i]==refs[i] for i in idxs)/len(idxs)*100\n",
    "        mt_c = meteor.compute(\n",
    "            predictions=[preds[i] for i in idxs],\n",
    "            references=[[refs[i]] for i in idxs]\n",
    "        )['meteor'] * 100\n",
    "        print(f\"{cat:7s} | EM: {em_c:5.2f}% | METEOR: {mt_c:5.2f}% | N={len(idxs)}\")\n",
    "\n",
    "    # per-spatial‐subtask\n",
    "    print(\"\\n-- Spatial Subtask Breakdown --\")\n",
    "    for cat in ['aggregation','placement','spatial','viewpoint']:\n",
    "        idxs = [i for i,s in enumerate(spat_types) if s==cat]\n",
    "        if not idxs: continue\n",
    "        em_c = sum(preds[i]==refs[i] for i in idxs)/len(idxs)*100\n",
    "        mt_c = meteor.compute(\n",
    "            predictions=[preds[i] for i in idxs],\n",
    "            references=[[refs[i]] for i in idxs]\n",
    "        )['meteor'] * 100\n",
    "        print(f\"{cat:11s} | EM: {em_c:5.2f}% | METEOR: {mt_c:5.2f}% | N={len(idxs)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9fGZIWk8EXrH"
   },
   "outputs": [],
   "source": [
    "# Cell 7: Run both methods\n",
    "evaluate_method(1)\n",
    "evaluate_method(2)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "google": {
   "image_path": "/static/site-assets/images/docs/logo-python.svg",
   "keywords": [
    "examples",
    "gemini",
    "beginner",
    "googleai",
    "quickstart",
    "python",
    "text",
    "chat",
    "vision",
    "embed"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
